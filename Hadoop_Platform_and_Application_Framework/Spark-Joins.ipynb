{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Platform and Application Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Join\n",
    "#### Week_4, Lesson_2\n",
    "Actual problem statement was for Hadoop MapReduce. Here, that assignment is instead done in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 2.1.0\n",
      "Spark App Name: Apache Toree"
     ]
    }
   ],
   "source": [
    "print(s\"Spark Version: ${spark.version}\\nSpark App Name: ${spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, sum}\n",
    "\n",
    "//Workaround in Toree to get SparkSession\n",
    "val sparkDummy = spark\n",
    "import sparkDummy.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|             show|count|\n",
      "+-----------------+-----+\n",
      "|    Hourly_Sports|   21|\n",
      "|      Hot_Talking|   44|\n",
      "|   Almost_Cooking|   91|\n",
      "|        Dumb_Show|  186|\n",
      "|PostModern_Sports|  377|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val join2Num = spark.read.\n",
    "    format(\"csv\").\n",
    "    load(\"file:///home/ubuntu/UCSD/hadoop-platform-and-application-framework/join2_gennum*.txt\").\n",
    "    as[(String, String)].\n",
    "    select('_c0 as 'show, '_c1.cast(\"int\") as 'count)\n",
    "\n",
    "join2Num.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join2Num.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|           show|channel|\n",
      "+---------------+-------+\n",
      "|  Hourly_Sports|    DEF|\n",
      "|    Hot_Cooking|    XYZ|\n",
      "| Almost_Talking|    CAB|\n",
      "|   Dumb_Talking|    MAN|\n",
      "|PostModern_News|    BAT|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val join2Chan = spark.read.\n",
    "    format(\"csv\").\n",
    "    load(\"file:///home/ubuntu/UCSD/hadoop-platform-and-application-framework/join2_genchan*.txt\").\n",
    "    as[(String, String)].\n",
    "    select('_c0 as 'show, '_c1 as 'channel)\n",
    "\n",
    "join2Chan.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join2Chan.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join2Num.createOrReplaceTempView(\"join2Num\")\n",
    "join2Chan.createOrReplaceTempView(\"join2Chan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[show: string, channel: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join2Num.cache()\n",
    "join2Chan.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            show| count|\n",
      "+----------------+------+\n",
      "|  Hourly_Talking|108163|\n",
      "|    Dumb_Talking|103894|\n",
      "|        Hot_Show| 54378|\n",
      "|  Hourly_Cooking| 54208|\n",
      "|       Dumb_Show| 53824|\n",
      "|     Cold_Sports| 52005|\n",
      "|     Baked_Games| 51604|\n",
      "|       Loud_Show| 50820|\n",
      "|PostModern_Games| 50644|\n",
      "|    Surreal_News| 50420|\n",
      "|       Hot_Games| 50228|\n",
      "|     Almost_Show| 50202|\n",
      "| PostModern_News| 50021|\n",
      "|      Loud_Games| 49482|\n",
      "|    Almost_Games| 49237|\n",
      "|     Hourly_Show| 48283|\n",
      "|       Cold_News| 47924|\n",
      "|      Baked_News| 47211|\n",
      "|  Surreal_Sports| 46834|\n",
      "|     Almost_News| 46592|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(s\"\"\"select join2Num.show as show, sum(count) as count \n",
    "    | from join2Num join join2Chan \n",
    "    | where join2Num.show = join2Chan.show and join2Chan.channel=\\\"ABC\\\" \n",
    "    | group by join2Num.show \n",
    "    | order by count desc\"\"\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+----------------+------+\n",
      "|            show| count|\n",
      "+----------------+------+\n",
      "|  Hourly_Talking|108163|\n",
      "|    Dumb_Talking|103894|\n",
      "|        Hot_Show| 54378|\n",
      "|  Hourly_Cooking| 54208|\n",
      "|       Dumb_Show| 53824|\n",
      "|     Cold_Sports| 52005|\n",
      "|     Baked_Games| 51604|\n",
      "|       Loud_Show| 50820|\n",
      "|PostModern_Games| 50644|\n",
      "|    Surreal_News| 50420|\n",
      "|       Hot_Games| 50228|\n",
      "|     Almost_Show| 50202|\n",
      "| PostModern_News| 50021|\n",
      "|      Loud_Games| 49482|\n",
      "|    Almost_Games| 49237|\n",
      "|     Hourly_Show| 48283|\n",
      "|       Cold_News| 47924|\n",
      "|      Baked_News| 47211|\n",
      "|  Surreal_Sports| 46834|\n",
      "|     Almost_News| 46592|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val joinCondition = (join2Num.col(\"show\") === join2Chan.col(\"show\")).\n",
    "    and(join2Chan.col(\"channel\") === \"ABC\")\n",
    "    \n",
    "val joinedDF = join2Num.join(join2Chan, joinCondition).\n",
    "    groupBy(join2Num.col(\"show\")).\n",
    "    agg(sum('count) as 'count).\n",
    "    sort($\"count\".desc).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Simple Join\n",
    "#### Week_5, Lesson_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  word|count1|\n",
      "+------+------+\n",
      "|  able|   991|\n",
      "| about|    11|\n",
      "|burger|    15|\n",
      "| actor|    22|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val simpleJoin1FileA = spark.read.\n",
    "    format(\"csv\").\n",
    "    load(\"file:///home/ubuntu/UCSD/hadoop-platform-and-application-framework/join1_FileA.txt\").\n",
    "    as[(String, String)].\n",
    "    select('_c0 as 'word, '_c1.cast(\"int\") as 'count1)\n",
    "    \n",
    "simpleJoin1FileA.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleJoin1FileA.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  word|  date|count2|\n",
      "+------+------+------+\n",
      "|  able|Jan-01|     5|\n",
      "| about|Feb-02|     3|\n",
      "| about|Mar-03|     8|\n",
      "|  able|Apr-04|    13|\n",
      "| actor|Feb-22|     3|\n",
      "|burger|Feb-23|     5|\n",
      "|burger|Mar-08|     2|\n",
      "|  able|Dec-15|   100|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val simpleJoin1FileB = spark.read.\n",
    "    format(\"csv\").\n",
    "    load(\"file:///home/ubuntu/UCSD/hadoop-platform-and-application-framework/join1_FileB.txt\").\n",
    "    map { word => \n",
    "        val one = word.toString.split(\",\"); \n",
    "        (one(0).toString.split(\" \")(1), one(0).toString.split(\" \")(0).replaceAll(\"\"\"\\[\"\"\", \"\"), one(1).replaceAll(\"\"\"\\]\"\"\", \"\"))\n",
    "    }.\n",
    "    toDF(\"word\", \"date\", \"count2\")\n",
    "\n",
    "simpleJoin1FileB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- count2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleJoin1FileB.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleJoin1FileA.createOrReplaceTempView(\"newA\")\n",
    "simpleJoin1FileB.createOrReplaceTempView(\"newB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+\n",
      "|  word|  date|count2|  word|count1|\n",
      "+------+------+------+------+------+\n",
      "|  able|Jan-01|     5|  able|   991|\n",
      "| about|Feb-02|     3| about|    11|\n",
      "| about|Mar-03|     8| about|    11|\n",
      "|  able|Apr-04|    13|  able|   991|\n",
      "| actor|Feb-22|     3| actor|    22|\n",
      "|burger|Feb-23|     5|burger|    15|\n",
      "|burger|Mar-08|     2|burger|    15|\n",
      "|  able|Dec-15|   100|  able|   991|\n",
      "+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val joinCondition = (simpleJoin1FileA.col(\"word\") === simpleJoin1FileB.col(\"word\"))\n",
    "\n",
    "val fileB_joined_fileA = simpleJoin1FileB.join(simpleJoin1FileA, joinCondition)\n",
    "fileB_joined_fileA.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-----+------+\n",
      "| word|  date|count2| word|count1|\n",
      "+-----+------+------+-----+------+\n",
      "|actor|Feb-22|     3|actor|    22|\n",
      "+-----+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(s\"\"\"select * from newB join newA\n",
    "    | where newA.word = newB.word and newA.word='actor'\n",
    "    \"\"\".stripMargin).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Advanced Join\n",
    "#### Week_5, Lesson_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-------+-------+\n",
      "|channel|  count|\n",
      "+-------+-------+\n",
      "|    DEF|8032799|\n",
      "|    MAN|6566187|\n",
      "|    XYZ|5208016|\n",
      "|    BAT|5099141|\n",
      "|    CNO|3941177|\n",
      "|    CAB|3940862|\n",
      "|    BOB|2591062|\n",
      "|    NOX|2583583|\n",
      "|    ABC|1115974|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(s\"\"\"select join2Chan.channel as channel, sum(count) as count \n",
    "    | from join2Num join join2Chan \n",
    "    | where join2Num.show = join2Chan.show\n",
    "    | group by join2Chan.channel \n",
    "    | order by count desc\n",
    "    \"\"\".stripMargin).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-------+-------+\n",
      "|channel|  count|\n",
      "+-------+-------+\n",
      "|    BAT|5099141|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val joinCondition = (join2Num.col(\"show\") === join2Chan.col(\"show\")).\n",
    "    and(join2Chan.col(\"channel\") === \"BAT\")\n",
    "    \n",
    "val joinedDF = join2Num.join(join2Chan, joinCondition).\n",
    "    groupBy(join2Chan.col(\"channel\"))\n",
    "\n",
    "joinedDF.agg(sum('count) as 'count).sort($\"count\".desc).show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
